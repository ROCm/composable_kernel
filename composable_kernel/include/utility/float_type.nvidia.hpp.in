#ifndef CK_FLOAT_TYPE_NVIDIA_HPP
#define CK_FLOAT_TYPE_NVIDIA_HPP

#include "number.hpp"

namespace ck {

// For some reason, CUDA need this definition, otherwise
//   compiler won't generate optimal load and store instruction, and
//   kernel would produce wrong result, indicating the compiler fail to generate correct
//   instruction,
// float
using float2_t = float2;
using float4_t = float4;

// float
typedef float float32_t __attribute__((ext_vector_type(32)));

// bfloat16
typedef ushort ushort2_t __attribute__((ext_vector_type(2)));
typedef ushort ushort4_t __attribute__((ext_vector_type(4)));
typedef ushort ushort8_t __attribute__((ext_vector_type(8)));

// fp16
using half_t  = half;
using half2_t = half2;
using half4_t = float2;

template <class T, index_t N>
struct vector_type
{
    typedef struct
    {
        T scalar[N];
    } type;
};

template <>
struct vector_type<float, 1>
{
    using type = float;

    template <index_t I>
    __host__ __device__ static void SetScalar(type& v, float s, Number<I>)
    {
        static_assert(I < 1, "wrong");
        *(reinterpret_cast<float*>(&v) + I) = s;
    }
};

template <>
struct vector_type<float, 2>
{
    using type = float2_t;

    union DataType
    {
        type vector;
        float scalar[2];
    };

    template <index_t I>
    __host__ __device__ static void SetScalar(type& v, float s, Number<I>)
    {
        static_assert(I < 2, "wrong");
        *(reinterpret_cast<float*>(&v) + I) = s;
    }

    __host__ __device__ static type Pack(float s0, float s1)
    {
        DataType data;
        data.scalar[0] = s0;
        data.scalar[1] = s1;
        return data.vector;
    }
};

template <>
struct vector_type<float, 4>
{
    using type = float4_t;

    __host__ __device__ static constexpr index_t GetSize() { return 4; }

    template <index_t I>
    __host__ __device__ static void SetScalar(type& v, float s, Number<I>)
    {
        static_assert(I < 4, "wrong");
        *(reinterpret_cast<float*>(&v) + I) = s;
    }
};

template <>
struct vector_type<half_t, 1>
{
    using type = half_t;

    template <index_t I>
    __host__ __device__ static void SetScalar(type& v, half_t s, Number<I>)
    {
        static_assert(I < 1, "wrong");
        *(reinterpret_cast<half_t*>(&v) + I) = s;
    }
};

template <>
struct vector_type<half_t, 2>
{
    using type = half2_t;

    union DataType
    {
        type vector;
        half_t scalar[2];
    };

    template <index_t I>
    __host__ __device__ static void SetScalar(type& v, half_t s, Number<I>)
    {
        static_assert(I < 2, "wrong");
        *(reinterpret_cast<half_t*>(&v) + I) = s;
    }

    __host__ __device__ static type Pack(half_t s0, half_t s1)
    {
        DataType data;
        data.scalar[0] = s0;
        data.scalar[1] = s1;
        return data.vector;
    }
};

// data type conversion
template <typename T>
struct type_convert
{
    template <typename X>
    __device__ T operator()(const X& x) const
    {
        return static_cast<T>(x);
    }
};

template <typename T>
struct inner_product_with_conversion
{
    static constexpr auto convert = type_convert<T>();

    __device__ T operator()(float a, float b) const { return convert(a) * convert(b); }

    __device__ T operator()(half2_t a, half2_t b) const
    {
        const half_t* p_a_half = reinterpret_cast<const half_t*>(&a);
        const half_t* p_b_half = reinterpret_cast<const half_t*>(&b);

        T acc = 0;
        for(index_t v = 0; v < 2; ++v)
        {
            acc += convert(p_a_half[v]) * convert(p_b_half[v]);
        }

        return acc;
    }

    __device__ T operator()(half4_t a, half4_t b) const
    {
        const half_t* p_a_half = reinterpret_cast<const half_t*>(&a);
        const half_t* p_b_half = reinterpret_cast<const half_t*>(&b);

        T acc = 0;
        for(index_t v = 0; v < 4; ++v)
        {
            acc += convert(p_a_half[v]) * convert(p_b_half[v]);
        }
        return acc;
    }
};

} // namespace ck
#endif
